{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c837d0f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T11:55:08.459909Z",
     "iopub.status.busy": "2025-05-26T11:55:08.458294Z",
     "iopub.status.idle": "2025-05-26T11:55:08.720850Z",
     "shell.execute_reply": "2025-05-26T11:55:08.719860Z",
     "shell.execute_reply.started": "2025-05-26T11:55:08.459850Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70e7f30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T11:55:09.252612Z",
     "iopub.status.busy": "2025-05-26T11:55:09.251232Z",
     "iopub.status.idle": "2025-05-26T11:55:09.259698Z",
     "shell.execute_reply": "2025-05-26T11:55:09.258040Z",
     "shell.execute_reply.started": "2025-05-26T11:55:09.252560Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2695e635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T11:55:10.223594Z",
     "iopub.status.busy": "2025-05-26T11:55:10.222744Z",
     "iopub.status.idle": "2025-05-26T11:55:23.792346Z",
     "shell.execute_reply": "2025-05-26T11:55:23.791275Z",
     "shell.execute_reply.started": "2025-05-26T11:55:10.223554Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/26 14:55:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/26 14:55:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "/home/santalovdv/.local/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello spark'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создание сессии с помощью SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()\n",
    "\n",
    "spark.sql(\"select 'hello spark'\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbcdc7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# создание ссесии с помощью SparkContext\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"newSession\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark.sql(\"select 'hello spark'\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4545c8f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T11:55:27.643849Z",
     "iopub.status.busy": "2025-05-26T11:55:27.642794Z",
     "iopub.status.idle": "2025-05-26T11:55:27.681699Z",
     "shell.execute_reply": "2025-05-26T11:55:27.680173Z",
     "shell.execute_reply.started": "2025-05-26T11:55:27.643802Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://oka-analyze-en-001.krd.bd-cloud.mts.ru:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f94cac72b10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0580b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# конвертирую excel в csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_excel = pd.read_excel('cvm_srv.xlsx')\n",
    "\n",
    "df_excel.to_csv('cvm_srv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943d8f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T11:55:32.330131Z",
     "iopub.status.busy": "2025-05-26T11:55:32.328496Z",
     "iopub.status.idle": "2025-05-26T11:55:34.169825Z",
     "shell.execute_reply": "2025-05-26T11:55:34.168338Z",
     "shell.execute_reply.started": "2025-05-26T11:55:32.330040Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# чтение фрейма данных\n",
    "\n",
    "# InferSchema - автоматическое определение типов данных при чтении файла\n",
    "\n",
    "df_spark = spark.read.option('header', 'true').csv('cvm_srv.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495f8e33",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-26T05:46:59.374913Z",
     "iopub.status.busy": "2025-05-26T05:46:59.373878Z",
     "iopub.status.idle": "2025-05-26T05:46:59.809419Z",
     "shell.execute_reply": "2025-05-26T05:46:59.808172Z",
     "shell.execute_reply.started": "2025-05-26T05:46:59.374858Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+--------------------+-----------+----------------+-----+---------+----------+------+--------+----+\n",
      "|campaign_cd|     campaign_nm|            name_cvm|global_code|     global_name|regid|  service|       cvm|contur| product|dayy|\n",
      "+-----------+----------------+--------------------+-----------+----------------+-----+---------+----------+------+--------+----+\n",
      "|CAMP1026252|1757_RTM_B2B_MNR|50 SMS в поездках...|   PE0910.1|100 SMS в Европе|   67|46355.067|CVMB2B-865|   b2c|Mob_core|  30|\n",
      "+-----------+----------------+--------------------+-----------+----------------+-----+---------+----------+------+--------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# вывод результата\n",
    "\n",
    "df_spark.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad588fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# вывод схемы\n",
    "\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce3070a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# получение колонок\n",
    "\n",
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40247b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# получение кортежа из строк\n",
    "df_spark.head(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb48aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Выборка по полю\n",
    "\n",
    "df_spark.select('regid', 'dayy').show()\n",
    "\n",
    "# df_spark['regid', 'dayy'].show()\n",
    "\n",
    "#df.regid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40531bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Добавление поля\n",
    "\n",
    "df_spark = df_spark.withColumn(\"new_column\", df_spark['regid']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53cdd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление поля\n",
    "\n",
    "df_spark = df_spark.drop('new_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переименование поля\n",
    "\n",
    "df_spark.withColumnRenamed(\"regid\", \"regid_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbf951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# удаление строк с пустыми полями\n",
    "\n",
    "# drop (how = как удалять, subset = из каких полей, thresh = какая то мера удаления)\n",
    "\n",
    "df_spark.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ba31c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# чем заполнить пробелы (Null)\n",
    "\n",
    "df_spark.na.fill('missing_values',['name_cvm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b4e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# заполнить пробелы средним значением\n",
    "\n",
    "# можно выбрать также mod, median\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols = [\"regid\",\"dayy\",\"new_column\"],\n",
    "                 outputCols = [\"{}_imputed\".format(c) for c in [\"regid\",\"dayy\",\"new_column\"]]).setStrategy(\"mean\") \n",
    "\n",
    "imputer.fit(df_spark).transform(df_spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188678f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# фильтры\n",
    "\n",
    "df_spark.filter(\"regid=68\").show()\n",
    "\n",
    "df_spark.filter(df_spark['regid']>68).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6038f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# фильтры с условием И (&) и ИЛИ (|)\n",
    "\n",
    "df_spark.filter((df_spark['regid']>68) & (df_spark['dayy'] == 70)).show()\n",
    "\n",
    "df_spark.filter((df_spark['regid']>68) | (df_spark['dayy'] == 70)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd3318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# фильтры с условием НЕ (~)\n",
    "\n",
    "df_spark.filter(~(df_spark['regid']>68)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d73ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтр like\n",
    "\n",
    "df_spark.filter(df_spark['global_code'].like('PE5%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d4071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# создание DataFrame (самостоятельно)\n",
    "\n",
    "data = [('Lucy', 10, 3_000),('Tanya', 35, 200_000), ('Kolya', 15, 0)]\n",
    "\n",
    "df = spark.createDataFrame(data, ['name', 'age', 'money'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группировка\n",
    "\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группировка\n",
    "# у группировки много есть агрегирующих функций (sum, min, max и тд.) Если не указывать ничего внутри функции, \n",
    "# он будет суммировать все поля числовые, если указать внутри функции, то только их\n",
    "\n",
    "df_spark.groupBy('global_code').sum('service', 'dayy')\n",
    "\n",
    "# df_spark.groupBy('global_code').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# среднее и подсчет элементов\n",
    "\n",
    "df_spark.groupBy('global_code').mean().show()\n",
    "\n",
    "df_spark.groupBy('global_code').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068eb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# агрегирующая функция (два варианта применения)\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df_spark.groupBy(\"global_code\").agg({'regid':'sum'}).show()\n",
    "\n",
    "df_spark.groupBy(\"global_code\").agg(sum('regid').alias('cnt_day')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db99daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сортировка \n",
    "\n",
    "\n",
    "# df_spark.sort(df_spark['dayy'].asc()).show()\n",
    "\n",
    "df_spark.orderBy(df_spark['dayy'].asc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5228308",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-26T05:50:07.733925Z",
     "iopub.status.busy": "2025-05-26T05:50:07.733031Z",
     "iopub.status.idle": "2025-05-26T05:50:07.774956Z",
     "shell.execute_reply": "2025-05-26T05:50:07.773645Z",
     "shell.execute_reply.started": "2025-05-26T05:50:07.733879Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 08:50:07 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[campaign_cd: string, campaign_nm: string, name_cvm: string, global_code: string, global_name: string, regid: int, service: double, cvm: string, contur: string, product: string, dayy: int]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сохранить датафрейм в памяти компьютера (для операций ускорения action операций)\n",
    "\n",
    "df_spark.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0871a3a-8e7f-420d-ae11-db94a57c72d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MEMORY_ONLY,MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2,MEMORY_AND_DISK_2\n",
    "# позволяет контролировать, где сохранится датафрейм, по умолчанию метод  MEMORY_AND_DISK\n",
    "\n",
    "df_spark.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80d72687-bef9-41e1-a242-8692fecddc53",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-26T06:03:59.440118Z",
     "iopub.status.busy": "2025-05-26T06:03:59.438305Z",
     "iopub.status.idle": "2025-05-26T06:03:59.689953Z",
     "shell.execute_reply": "2025-05-26T06:03:59.688518Z",
     "shell.execute_reply.started": "2025-05-26T06:03:59.440080Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+--------------------+-----------+--------------------+-----+----------+-----------+------+--------+----+\n",
      "|campaign_cd|       campaign_nm|            name_cvm|global_code|         global_name|regid|   service|        cvm|contur| product|dayy|\n",
      "+-----------+------------------+--------------------+-----------+--------------------+-----+----------+-----------+------+--------+----+\n",
      "|CAMP1026252|  1757_RTM_B2B_MNR|50 SMS в поездках...|   PE1094.1|150 SMS в поездка...|   71| 76432.071| CVMB2B-865|   b2c|Mob_core|  30|\n",
      "|CAMP1026252|  1757_RTM_B2B_MNR|50 SMS в поездках...|   PE0909.1|     50 SMS в Европе|   71| 75320.071| CVMB2B-865|   b2c|Mob_core|  30|\n",
      "|CAMP1026252|  1757_RTM_B2B_MNR|50 SMS в поездках...|   PE1095.1|    150 SMS в Европе|   71| 76435.071| CVMB2B-865|   b2c|Mob_core|  30|\n",
      "|CAMP1026252|  1757_RTM_B2B_MNR|50 SMS в поездках...|   PE0540.1|50 SMS в поездках...|   71| 66971.071| CVMB2B-865|   b2c|Mob_core|  30|\n",
      "|CAMP1026252|  1757_RTM_B2B_MNR|50 SMS в поездках...|   PE0909.1|     50 SMS в Европе|   71| 75320.071| CVMB2B-865|   b2c|Mob_core|  30|\n",
      "|CAMP1026252|  1757_RTM_B2B_MNR|50 SMS в поездках...|   PE0910.1|    100 SMS в Европе|   71| 75323.071| CVMB2B-865|   b2c|Mob_core|  30|\n",
      "|CAMP1026252|  1757_RTM_B2B_MNR|50 SMS в поездках...|   PE1096.1|100 SMS в поездка...|   71| 43449.071| CVMB2B-865|   b2c|Mob_core|  30|\n",
      "|CAMP1026252|  1757_RTM_B2B_MNR|50 SMS в поездках...|   PE0540.1|50 SMS в поездках...|   71| 66971.071| CVMB2B-865|   b2c|Mob_core|  30|\n",
      "|CAMP1026681|   B2B_202408_KION|                KION|  PE23890.1|  KION корпоративный|   71|142159.071|CVMB2B-1006|   b2c|Mob_core|  30|\n",
      "|CAMP1024632|B2B_202403KIONCOMP|                KION|  PE23890.1|  KION корпоративный|   71|142159.071| CVMB2B-854|   b2c|Mob_core|  30|\n",
      "+-----------+------------------+--------------------+-----------+--------------------+-----+----------+-----------+------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# применения функции col (для выделения полей отдельной функцией)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_spark.filter((col('global_code').like('PE%')) & \\\n",
    "         (col('regid') > 68)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54efcc7e-f566-4800-8624-343b498b4ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T12:09:45.312484Z",
     "iopub.status.busy": "2025-05-26T12:09:45.311452Z",
     "iopub.status.idle": "2025-05-26T12:09:46.394287Z",
     "shell.execute_reply": "2025-05-26T12:09:46.393056Z",
     "shell.execute_reply.started": "2025-05-26T12:09:45.312419Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4943/2562350903.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Присоединяем df2 к df1 по \"Id\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2978\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 2980\u001b[0;31m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m             )\n\u001b[1;32m   2982\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'col'"
     ]
    }
   ],
   "source": [
    "# JOIN в pypsark. В параметр how можно передать тип join-a \n",
    "\n",
    "# \"inner\" (внутреннее): возвращает строки, которые есть в обоих DataFrame.\n",
    "# \"outer\" (внешнее) или \"full\" (полное): возвращает все строки из обоих DataFrame.\n",
    "# \"left\" (левое): возвращает все строки из левого DataFrame и совпадающие строки из правого DataFrame.\n",
    "# \"right\" (правое): возвращает все строки из правого DataFrame и совпадающие строки из левого DataFrame.\n",
    "# \"semi\" (полу): возвращает строки из левого DataFrame, где есть совпадения в правом DataFrame.\n",
    "# \"anti\" (анти): возвращает строки из левого DataFrame, где нет совпадений в правом DataFrame.\n",
    "\n",
    "\n",
    "df1 = spark.createDataFrame([(1, \"John\", \"Chicago\"),\n",
    "                            (2, \"Mike\", \"New York\"),\n",
    "                            (3, \"Sue\", \"Washington\")], [\"Id\", \"Name\", \"City\"])\n",
    "\n",
    "df2 = spark.createDataFrame([(1, \"Blue\"),\n",
    "                            (2, \"Red\"),\n",
    "                            (4, \"Green\")], [\"Id\", \"Color\"])\n",
    "\n",
    "# Присоединяем df2 к df1 по \"Id\"\n",
    "df3 = df1.join(df2, on=\"Id\", how=\"inner\").select('Name')\n",
    "\n",
    "# df3 = df1.join(df2, df1.Id == df2.UserId, how=\"inner\")\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8516a0-bcdc-4fc5-ad4e-d7d662579f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
