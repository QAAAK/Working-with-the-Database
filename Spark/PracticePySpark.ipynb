{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c837d0f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T11:00:00.634864Z",
     "iopub.status.busy": "2025-05-27T11:00:00.629243Z",
     "iopub.status.idle": "2025-05-27T11:00:00.808373Z",
     "shell.execute_reply": "2025-05-27T11:00:00.807143Z",
     "shell.execute_reply.started": "2025-05-27T11:00:00.634808Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b70e7f30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T11:00:01.328592Z",
     "iopub.status.busy": "2025-05-27T11:00:01.326991Z",
     "iopub.status.idle": "2025-05-27T11:00:01.335192Z",
     "shell.execute_reply": "2025-05-27T11:00:01.333651Z",
     "shell.execute_reply.started": "2025-05-27T11:00:01.328522Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2695e635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T11:00:01.912862Z",
     "iopub.status.busy": "2025-05-27T11:00:01.911601Z",
     "iopub.status.idle": "2025-05-27T11:00:14.438117Z",
     "shell.execute_reply": "2025-05-27T11:00:14.435386Z",
     "shell.execute_reply.started": "2025-05-27T11:00:01.912812Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/27 14:00:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello spark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создание сессии с помощью SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()\n",
    "\n",
    "spark.sql(\"select 'hello spark'\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbcdc7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# создание ссесии с помощью SparkContext\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"newSession\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark.sql(\"select 'hello spark'\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545c8f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0580b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# конвертирую excel в csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_excel = pd.read_excel('cvm_srv.xlsx')\n",
    "\n",
    "df_excel.to_csv('cvm_srv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "943d8f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T11:00:18.448902Z",
     "iopub.status.busy": "2025-05-27T11:00:18.448288Z",
     "iopub.status.idle": "2025-05-27T11:00:20.175686Z",
     "shell.execute_reply": "2025-05-27T11:00:20.173291Z",
     "shell.execute_reply.started": "2025-05-27T11:00:18.448854Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# чтение фрейма данных\n",
    "\n",
    "# InferSchema - автоматическое определение типов данных при чтении файла\n",
    "\n",
    "df_spark = spark.read.option('header', 'true').csv('cvm_srv.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f8e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# вывод результата\n",
    "\n",
    "df_spark.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad588fb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T11:00:25.572566Z",
     "iopub.status.busy": "2025-05-27T11:00:25.572027Z",
     "iopub.status.idle": "2025-05-27T11:00:25.653420Z",
     "shell.execute_reply": "2025-05-27T11:00:25.644298Z",
     "shell.execute_reply.started": "2025-05-27T11:00:25.572522Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- campaign_cd: string (nullable = true)\n",
      " |-- campaign_nm: string (nullable = true)\n",
      " |-- name_cvm: string (nullable = true)\n",
      " |-- global_code: string (nullable = true)\n",
      " |-- global_name: string (nullable = true)\n",
      " |-- regid: integer (nullable = true)\n",
      " |-- service: double (nullable = true)\n",
      " |-- cvm: string (nullable = true)\n",
      " |-- contur: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- dayy: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# вывод схемы\n",
    "\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce3070a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T11:00:30.609053Z",
     "iopub.status.busy": "2025-05-27T11:00:30.607455Z",
     "iopub.status.idle": "2025-05-27T11:00:30.624077Z",
     "shell.execute_reply": "2025-05-27T11:00:30.622925Z",
     "shell.execute_reply.started": "2025-05-27T11:00:30.608982Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['campaign_cd',\n",
       " 'campaign_nm',\n",
       " 'name_cvm',\n",
       " 'global_code',\n",
       " 'global_name',\n",
       " 'regid',\n",
       " 'service',\n",
       " 'cvm',\n",
       " 'contur',\n",
       " 'product',\n",
       " 'dayy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# получение колонок\n",
    "\n",
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40247b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# получение кортежа из строк\n",
    "df_spark.head(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb48aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Выборка по полю\n",
    "\n",
    "df_spark.select('regid', 'dayy').show()\n",
    "\n",
    "# df_spark['regid', 'dayy'].show()\n",
    "\n",
    "#df.regid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40531bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Добавление поля\n",
    "\n",
    "df_spark = df_spark.withColumn(\"new_column\", df_spark['regid']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53cdd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление поля\n",
    "\n",
    "df_spark = df_spark.drop('new_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переименование поля\n",
    "\n",
    "df_spark.withColumnRenamed(\"regid\", \"regid_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbf951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# удаление строк с пустыми полями\n",
    "\n",
    "# drop (how = как удалять, subset = из каких полей, thresh = какая то мера удаления)\n",
    "\n",
    "df_spark.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ba31c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# чем заполнить пробелы (Null)\n",
    "\n",
    "df_spark.na.fill('missing_values',['name_cvm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b4e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# заполнить пробелы средним значением\n",
    "\n",
    "# можно выбрать также mod, median\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols = [\"regid\",\"dayy\",\"new_column\"],\n",
    "                 outputCols = [\"{}_imputed\".format(c) for c in [\"regid\",\"dayy\",\"new_column\"]]).setStrategy(\"mean\") \n",
    "\n",
    "imputer.fit(df_spark).transform(df_spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188678f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# фильтры\n",
    "\n",
    "df_spark.filter(\"regid=68\").show()\n",
    "\n",
    "df_spark.filter(df_spark['regid']>68).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6038f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# фильтры с условием И (&) и ИЛИ (|)\n",
    "\n",
    "df_spark.filter((df_spark['regid']>68) & (df_spark['dayy'] == 70)).show()\n",
    "\n",
    "df_spark.filter((df_spark['regid']>68) | (df_spark['dayy'] == 70)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd3318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# фильтры с условием НЕ (~)\n",
    "\n",
    "df_spark.filter(~(df_spark['regid']>68)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d73ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтр like\n",
    "\n",
    "df_spark.filter(df_spark['global_code'].like('PE5%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d4071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# создание DataFrame (самостоятельно)\n",
    "\n",
    "data = [('Lucy', 10, 3_000),('Tanya', 35, 200_000), ('Kolya', 15, 0)]\n",
    "\n",
    "df = spark.createDataFrame(data, ['name', 'age', 'money'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группировка\n",
    "\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группировка\n",
    "# у группировки много есть агрегирующих функций (sum, min, max и тд.) Если не указывать ничего внутри функции, \n",
    "# он будет суммировать все поля числовые, если указать внутри функции, то только их\n",
    "\n",
    "df_spark.groupBy('global_code').sum('service', 'dayy')\n",
    "\n",
    "# df_spark.groupBy('global_code').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# среднее и подсчет элементов\n",
    "\n",
    "df_spark.groupBy('global_code').mean().show()\n",
    "\n",
    "df_spark.groupBy('global_code').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068eb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# агрегирующая функция (два варианта применения)\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df_spark.groupBy(\"global_code\").agg({'regid':'sum'}).show()\n",
    "\n",
    "df_spark.groupBy(\"global_code\").agg(sum('regid').alias('cnt_day')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db99daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сортировка \n",
    "\n",
    "\n",
    "# df_spark.sort(df_spark['dayy'].asc()).show()\n",
    "\n",
    "df_spark.orderBy(df_spark['dayy'].asc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5228308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# сохранить датафрейм в памяти компьютера (для операций ускорения action операций)\n",
    "\n",
    "df_spark.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0871a3a-8e7f-420d-ae11-db94a57c72d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MEMORY_ONLY,MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2,MEMORY_AND_DISK_2\n",
    "# позволяет контролировать, где сохранится датафрейм, по умолчанию метод  MEMORY_AND_DISK\n",
    "\n",
    "df_spark.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d72687-bef9-41e1-a242-8692fecddc53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# применения функции col (для выделения полей отдельной функцией)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_spark.filter((col('global_code').like('PE%')) & \\\n",
    "         (col('regid') > 68)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efcc7e-f566-4800-8624-343b498b4ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# JOIN в pypsark. В параметр how можно передать тип join-a \n",
    "\n",
    "# \"inner\" (внутреннее): возвращает строки, которые есть в обоих DataFrame.\n",
    "# \"outer\" (внешнее) или \"full\" (полное): возвращает все строки из обоих DataFrame.\n",
    "# \"left\" (левое): возвращает все строки из левого DataFrame и совпадающие строки из правого DataFrame.\n",
    "# \"right\" (правое): возвращает все строки из правого DataFrame и совпадающие строки из левого DataFrame.\n",
    "# \"semi\" (полу): возвращает строки из левого DataFrame, где есть совпадения в правом DataFrame.\n",
    "# \"anti\" (анти): возвращает строки из левого DataFrame, где нет совпадений в правом DataFrame.\n",
    "\n",
    "\n",
    "df1 = spark.createDataFrame([(1, \"John\", \"Chicago\"),\n",
    "                            (2, \"Mike\", \"New York\"),\n",
    "                            (3, \"Sue\", \"Washington\")], [\"Id\", \"Name\", \"City\"])\n",
    "\n",
    "df2 = spark.createDataFrame([(1, \"Blue\"),\n",
    "                            (2, \"Red\"),\n",
    "                            (4, \"Green\")], [\"Id\", \"Color\"])\n",
    "\n",
    "# Присоединяем df2 к df1 по \"Id\"\n",
    "df3 = df1.join(df2, on=\"Id\", how=\"inner\").select('Name')\n",
    "\n",
    "# df3 = df1.join(df2, df1.Id == df2.UserId, how=\"inner\")\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d28cc416-4861-42b1-ac2c-ebd8fe2ac569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T11:02:53.654736Z",
     "iopub.status.busy": "2025-05-27T11:02:53.653848Z",
     "iopub.status.idle": "2025-05-27T11:02:53.715018Z",
     "shell.execute_reply": "2025-05-27T11:02:53.713270Z",
     "shell.execute_reply.started": "2025-05-27T11:02:53.654662Z"
    }
   },
   "outputs": [],
   "source": [
    "# Приведение типов данных \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "df_spark_string = df_spark.select(col('dayy').cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f687f42-e541-41e9-988d-5951f14539b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8-default",
   "language": "python",
   "name": "py38-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
