{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837d0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e7f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2695e635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# создание сессии с помощью SparkSession\n",
    "\n",
    "\n",
    "# метод .config() - при создании сессии, метод .set() - после создания\n",
    "\n",
    "\n",
    "# НАСТРОЙКА СТАТИЧЕСКОЙ АЛЛОКАЦИИ\n",
    " # .set(\"spark.driver.memory\", \"2g\")\n",
    " #            .set(\"spark.driver.cores\", 2) #Задаем только в Cluster Mode  \n",
    " #            .set(\"spark.executor.cores\", 5) \n",
    " #            .set(\"spark.executor.instances\", 3) \n",
    " #            .set(\"spark.dynamicAllocation.enabled\",'false')\n",
    " #            .set(\"spark.master\", \"yarn\")\n",
    " #            .set(\"spark.submit.deploymode\", \"client\")\n",
    "\n",
    "# НАСТРОЙКА ДИНАМИЧЕСКОЙ АЛЛОКАЦИИ\n",
    "            # .set(\"spark.driver.memory\", \"2g\")\n",
    "            # .set(\"spark.driver.cores\", 2) #Задаем только в Cluster Mode \n",
    "            # .set(\"spark.executor.cores\", 5) \n",
    "            # .set(\"spark.submit.deploymode\", \"client\")\n",
    "            # .set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "            # .set(\"spark.dynamicAllocation.initialExecutors\", \"1\")\n",
    "            # .set(\"spark.dynamicAllocation.minExecutors\", \"0\")\n",
    "            # .set(\"spark.dynamicAllocation.maxExecutors\", \"3\")\n",
    "            # .set(\"spark.dynamicAllocation.executorIdleTimeout\", \"360s\")\n",
    "\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()\n",
    "\n",
    "spark.sql(\"select 'hello spark'\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbcdc7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# создание ссесии с помощью SparkContext\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"newSession\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark.sql(\"select 'hello spark'\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545c8f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0580b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# конвертирую excel в csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_excel = pd.read_excel('cvm_srv.xlsx')\n",
    "\n",
    "df_excel.to_csv('cvm_srv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d8f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# чтение фрейма данных\n",
    "\n",
    "# InferSchema - автоматическое определение типов данных при чтении файла\n",
    "\n",
    "df_spark = spark.read.option('header', 'true').csv('cvm_srv.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f8e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# вывод результата\n",
    "\n",
    "df_spark.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad588fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# вывод схемы\n",
    "\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce3070a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# получение колонок\n",
    "\n",
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40247b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# получение кортежа из строк\n",
    "\n",
    "df_spark.head(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb48aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Выборка по полю\n",
    "\n",
    "df_spark.select('regid', 'dayy').show()\n",
    "\n",
    "# df_spark['regid', 'dayy'].show()\n",
    "\n",
    "#df.regid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40531bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Добавление поля\n",
    "\n",
    "df_spark = df_spark.withColumn(\"new_column\", df_spark['regid']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f92711c-11c8-46e9-91ac-6d66773b10da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# добавление поля с константным выражением\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_spark = df_spark.withColumn(\"const_column\", lit(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53cdd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление поля\n",
    "\n",
    "df_spark = df_spark.drop('new_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переименование поля\n",
    "\n",
    "df_spark.withColumnRenamed(\"regid\", \"regid_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbf951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# удаление строк с пустыми полями\n",
    "\n",
    "# drop (how = как удалять, subset = из каких полей, thresh = какая то мера удаления)\n",
    "\n",
    "df_spark.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ba31c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# чем заполнить пробелы (Null)\n",
    "\n",
    "df_spark.na.fill('missing_values',['name_cvm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b4e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# заполнить пробелы средним значением\n",
    "\n",
    "# можно выбрать также mod, median\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols = [\"regid\",\"dayy\",\"new_column\"],\n",
    "                 outputCols = [\"{}_imputed\".format(c) for c in [\"regid\",\"dayy\",\"new_column\"]]).setStrategy(\"mean\") \n",
    "\n",
    "imputer.fit(df_spark).transform(df_spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188678f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# фильтры\n",
    "\n",
    "df_spark.filter(\"regid=68\").show()\n",
    "\n",
    "df_spark.filter(df_spark['regid']>68).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6038f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# фильтры с условием И (&) и ИЛИ (|)\n",
    "\n",
    "df_spark.filter((df_spark['regid']>68) & (df_spark['dayy'] == 70)).show()\n",
    "\n",
    "df_spark.filter((df_spark['regid']>68) | (df_spark['dayy'] == 70)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd3318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# фильтры с условием НЕ (~)\n",
    "\n",
    "df_spark.filter(~(df_spark['regid']>68)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d73ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтр like\n",
    "\n",
    "df_spark.filter(df_spark['global_code'].like('PE5%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d4071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# создание DataFrame (самостоятельно)\n",
    "\n",
    "data = [('Lucy', 10, 3_000),('Tanya', 35, 200_000), ('Kolya', 15, 0)]\n",
    "\n",
    "df = spark.createDataFrame(data, ['name', 'age', 'money'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группировка\n",
    "\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группировка\n",
    "# у группировки много есть агрегирующих функций (sum, min, max и тд.) Если не указывать ничего внутри функции, \n",
    "# он будет суммировать все поля числовые, если указать внутри функции, то только их\n",
    "\n",
    "df_spark.groupBy('global_code').sum('service', 'dayy')\n",
    "\n",
    "# df_spark.groupBy('global_code').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# среднее и подсчет элементов\n",
    "\n",
    "df_spark.groupBy('global_code').mean().show()\n",
    "\n",
    "df_spark.groupBy('global_code').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068eb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# агрегирующая функция (два варианта применения)\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df_spark.groupBy(\"global_code\").agg({'regid':'sum'}).show()\n",
    "\n",
    "df_spark.groupBy(\"global_code\").agg(sum('regid').alias('cnt_day')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db99daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сортировка \n",
    "\n",
    "\n",
    "# df_spark.sort(df_spark['dayy'].asc()).show()\n",
    "\n",
    "df_spark.orderBy(df_spark['dayy'].asc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5228308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# сохранить датафрейм в памяти компьютера (для операций ускорения action операций)\n",
    "\n",
    "df_spark.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0871a3a-8e7f-420d-ae11-db94a57c72d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MEMORY_ONLY,MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2,MEMORY_AND_DISK_2\n",
    "# позволяет контролировать, где сохранится датафрейм, по умолчанию метод  MEMORY_AND_DISK\n",
    "\n",
    "df_spark.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d72687-bef9-41e1-a242-8692fecddc53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# применения функции col (для выделения полей отдельной функцией)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_spark.filter((col('global_code').like('PE%')) & \\\n",
    "         (col('regid') > 68)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efcc7e-f566-4800-8624-343b498b4ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# JOIN в pypsark. В параметр how можно передать тип join-a \n",
    "\n",
    "# \"inner\" (внутреннее): возвращает строки, которые есть в обоих DataFrame.\n",
    "# \"outer\" (внешнее) или \"full\" (полное): возвращает все строки из обоих DataFrame.\n",
    "# \"left\" (левое): возвращает все строки из левого DataFrame и совпадающие строки из правого DataFrame.\n",
    "# \"right\" (правое): возвращает все строки из правого DataFrame и совпадающие строки из левого DataFrame.\n",
    "# \"semi\" (полу): возвращает строки из левого DataFrame, где есть совпадения в правом DataFrame.\n",
    "# \"anti\" (анти): возвращает строки из левого DataFrame, где нет совпадений в правом DataFrame.\n",
    "\n",
    "\n",
    "df1 = spark.createDataFrame([(1, \"John\", \"Chicago\"),\n",
    "                            (2, \"Mike\", \"New York\"),\n",
    "                            (3, \"Sue\", \"Washington\")], [\"Id\", \"Name\", \"City\"])\n",
    "\n",
    "df2 = spark.createDataFrame([(1, \"Blue\"),\n",
    "                            (2, \"Red\"),\n",
    "                            (4, \"Green\")], [\"Id\", \"Color\"])\n",
    "\n",
    "# Присоединяем df2 к df1 по \"Id\"\n",
    "df3 = df1.join(df2, on=\"Id\", how=\"inner\").select('Name')\n",
    "\n",
    "# df3 = df1.join(df2, df1.Id == df2.UserId, how=\"inner\")\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28cc416-4861-42b1-ac2c-ebd8fe2ac569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Приведение типов данных \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "df_spark_string = df_spark.select(col('dayy').cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf07d7-6d17-453f-a7d8-491ffd53ff50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# получить статистику по таблице\n",
    "\n",
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10cc83-128a-49c9-bcd0-335b8bab8d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# получить уникальный фрейм данных \n",
    "\n",
    "df_spark.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f3772-c6e3-4224-bdd4-eedd7cb5d3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8-default",
   "language": "python",
   "name": "py38-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
