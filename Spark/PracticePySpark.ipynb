{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c837d0f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:44:28.765371Z",
     "iopub.status.busy": "2025-06-03T06:44:28.763849Z",
     "iopub.status.idle": "2025-06-03T06:44:28.913768Z",
     "shell.execute_reply": "2025-06-03T06:44:28.912527Z",
     "shell.execute_reply.started": "2025-06-03T06:44:28.765302Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b70e7f30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:44:29.729516Z",
     "iopub.status.busy": "2025-06-03T06:44:29.727937Z",
     "iopub.status.idle": "2025-06-03T06:44:29.735530Z",
     "shell.execute_reply": "2025-06-03T06:44:29.734101Z",
     "shell.execute_reply.started": "2025-06-03T06:44:29.729445Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2695e635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:44:32.745675Z",
     "iopub.status.busy": "2025-06-03T06:44:32.744774Z",
     "iopub.status.idle": "2025-06-03T06:44:45.604266Z",
     "shell.execute_reply": "2025-06-03T06:44:45.602671Z",
     "shell.execute_reply.started": "2025-06-03T06:44:32.745634Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/03 09:44:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/03 09:44:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello spark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создание сессии с помощью SparkSession\n",
    "\n",
    "\n",
    "# метод .config() - при создании сессии, метод .set() - после создания\n",
    "\n",
    "\n",
    "# НАСТРОЙКА СТАТИЧЕСКОЙ АЛЛОКАЦИИ\n",
    " # .set(\"spark.driver.memory\", \"2g\")\n",
    " #            .set(\"spark.driver.cores\", 2) #Задаем только в Cluster Mode  \n",
    " #            .set(\"spark.executor.cores\", 5) \n",
    " #            .set(\"spark.executor.instances\", 3) \n",
    " #            .set(\"spark.dynamicAllocation.enabled\",'false')\n",
    " #            .set(\"spark.master\", \"yarn\")\n",
    " #            .set(\"spark.submit.deploymode\", \"client\")\n",
    "\n",
    "# НАСТРОЙКА ДИНАМИЧЕСКОЙ АЛЛОКАЦИИ\n",
    "            # .set(\"spark.driver.memory\", \"2g\")\n",
    "            # .set(\"spark.driver.cores\", 2) #Задаем только в Cluster Mode \n",
    "            # .set(\"spark.executor.cores\", 5) \n",
    "            # .set(\"spark.submit.deploymode\", \"client\")\n",
    "            # .set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "            # .set(\"spark.dynamicAllocation.initialExecutors\", \"1\")\n",
    "            # .set(\"spark.dynamicAllocation.minExecutors\", \"0\")\n",
    "            # .set(\"spark.dynamicAllocation.maxExecutors\", \"3\")\n",
    "            # .set(\"spark.dynamicAllocation.executorIdleTimeout\", \"360s\")\n",
    "\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()\n",
    "\n",
    "spark.sql(\"select 'hello spark'\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbcdc7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# создание ссесии с помощью SparkContext\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"newSession\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark.sql(\"select 'hello spark'\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545c8f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0580b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# конвертирую excel в csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_excel = pd.read_excel('cvm_srv.xlsx')\n",
    "\n",
    "df_excel.to_csv('cvm_srv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "943d8f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:45:05.579919Z",
     "iopub.status.busy": "2025-06-03T06:45:05.579063Z",
     "iopub.status.idle": "2025-06-03T06:45:07.213114Z",
     "shell.execute_reply": "2025-06-03T06:45:07.211915Z",
     "shell.execute_reply.started": "2025-06-03T06:45:05.579862Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# чтение фрейма данных\n",
    "\n",
    "# InferSchema - автоматическое определение типов данных при чтении файла\n",
    "\n",
    "df_spark = spark.read.option('header', 'true').csv('cvm_srv.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f8e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# вывод результата\n",
    "\n",
    "df_spark.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad588fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# вывод схемы\n",
    "\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce3070a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# получение колонок\n",
    "\n",
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40247b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# получение кортежа из строк\n",
    "\n",
    "df_spark.head(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb48aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Выборка по полю\n",
    "\n",
    "df_spark.select('regid', 'dayy').show()\n",
    "\n",
    "# df_spark['regid', 'dayy'].show()\n",
    "\n",
    "#df.regid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40531bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Добавление поля\n",
    "\n",
    "df_spark = df_spark.withColumn(\"new_column\", df_spark['regid']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f92711c-11c8-46e9-91ac-6d66773b10da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# добавление поля с константным выражением\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_spark = df_spark.withColumn(\"const_column\", lit(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53cdd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление поля\n",
    "\n",
    "df_spark = df_spark.drop('new_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переименование поля\n",
    "\n",
    "df_spark.withColumnRenamed(\"regid\", \"regid_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbf951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# удаление строк с пустыми полями\n",
    "\n",
    "# drop (how = как удалять, subset = из каких полей, thresh = какая то мера удаления)\n",
    "\n",
    "df_spark.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ba31c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# чем заполнить пробелы (Null)\n",
    "\n",
    "df_spark.na.fill('missing_values',['name_cvm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b4e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# заполнить пробелы средним значением\n",
    "\n",
    "# можно выбрать также mod, median\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols = [\"regid\",\"dayy\",\"new_column\"],\n",
    "                 outputCols = [\"{}_imputed\".format(c) for c in [\"regid\",\"dayy\",\"new_column\"]]).setStrategy(\"mean\") \n",
    "\n",
    "imputer.fit(df_spark).transform(df_spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188678f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# фильтры\n",
    "\n",
    "df_spark.filter(\"regid=68\").show()\n",
    "\n",
    "df_spark.filter(df_spark['regid']>68).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6038f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# фильтры с условием И (&) и ИЛИ (|)\n",
    "\n",
    "df_spark.filter((df_spark['regid']>68) & (df_spark['dayy'] == 70)).show()\n",
    "\n",
    "df_spark.filter((df_spark['regid']>68) | (df_spark['dayy'] == 70)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd3318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# фильтры с условием НЕ (~)\n",
    "\n",
    "df_spark.filter(~(df_spark['regid']>68)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d73ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтр like\n",
    "\n",
    "df_spark.filter(df_spark['global_code'].like('PE5%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d4071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# создание DataFrame (самостоятельно)\n",
    "\n",
    "data = [('Lucy', 10, 3_000),('Tanya', 35, 200_000), ('Kolya', 15, 0)]\n",
    "\n",
    "df = spark.createDataFrame(data, ['name', 'age', 'money'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группировка\n",
    "\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группировка\n",
    "# у группировки много есть агрегирующих функций (sum, min, max и тд.) Если не указывать ничего внутри функции, \n",
    "# он будет суммировать все поля числовые, если указать внутри функции, то только их\n",
    "\n",
    "df_spark.groupBy('global_code').sum('service', 'dayy')\n",
    "\n",
    "# df_spark.groupBy('global_code').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# среднее и подсчет элементов\n",
    "\n",
    "df_spark.groupBy('global_code').mean().show()\n",
    "\n",
    "df_spark.groupBy('global_code').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068eb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# агрегирующая функция (два варианта применения)\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df_spark.groupBy(\"global_code\").agg({'regid':'sum'}).show()\n",
    "\n",
    "df_spark.groupBy(\"global_code\").agg(sum('regid').alias('cnt_day')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db99daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сортировка \n",
    "\n",
    "\n",
    "# df_spark.sort(df_spark['dayy'].asc()).show()\n",
    "\n",
    "df_spark.orderBy(df_spark['dayy'].asc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5228308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# сохранить датафрейм в памяти компьютера (для операций ускорения action операций)\n",
    "\n",
    "df_spark.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0871a3a-8e7f-420d-ae11-db94a57c72d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MEMORY_ONLY,MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2,MEMORY_AND_DISK_2\n",
    "# позволяет контролировать, где сохранится датафрейм, по умолчанию метод  MEMORY_AND_DISK\n",
    "\n",
    "df_spark.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d72687-bef9-41e1-a242-8692fecddc53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# применения функции col (для выделения полей отдельной функцией)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_spark.filter((col('global_code').like('PE%')) & \\\n",
    "         (col('regid') > 68)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efcc7e-f566-4800-8624-343b498b4ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# JOIN в pypsark. В параметр how можно передать тип join-a \n",
    "\n",
    "# \"inner\" (внутреннее): возвращает строки, которые есть в обоих DataFrame.\n",
    "# \"outer\" (внешнее) или \"full\" (полное): возвращает все строки из обоих DataFrame.\n",
    "# \"left\" (левое): возвращает все строки из левого DataFrame и совпадающие строки из правого DataFrame.\n",
    "# \"right\" (правое): возвращает все строки из правого DataFrame и совпадающие строки из левого DataFrame.\n",
    "# \"semi\" (полу): возвращает строки из левого DataFrame, где есть совпадения в правом DataFrame.\n",
    "# \"anti\" (анти): возвращает строки из левого DataFrame, где нет совпадений в правом DataFrame.\n",
    "\n",
    "\n",
    "df1 = spark.createDataFrame([(1, \"John\", \"Chicago\"),\n",
    "                            (2, \"Mike\", \"New York\"),\n",
    "                            (3, \"Sue\", \"Washington\")], [\"Id\", \"Name\", \"City\"])\n",
    "\n",
    "df2 = spark.createDataFrame([(1, \"Blue\"),\n",
    "                            (2, \"Red\"),\n",
    "                            (4, \"Green\")], [\"Id\", \"Color\"])\n",
    "\n",
    "# Присоединяем df2 к df1 по \"Id\"\n",
    "df3 = df1.join(df2, on=\"Id\", how=\"inner\").select('Name')\n",
    "\n",
    "# df3 = df1.join(df2, df1.Id == df2.UserId, how=\"inner\")\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28cc416-4861-42b1-ac2c-ebd8fe2ac569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Приведение типов данных \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "df_spark_string = df_spark.select(col('dayy').cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3daf07d7-6d17-453f-a7d8-491ffd53ff50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T07:10:43.370034Z",
     "iopub.status.busy": "2025-06-03T07:10:43.368625Z",
     "iopub.status.idle": "2025-06-03T07:10:44.538823Z",
     "shell.execute_reply": "2025-06-03T07:10:44.537276Z",
     "shell.execute_reply.started": "2025-06-03T07:10:43.369981Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+--------------------+---------------+--------------------+------------------+------------------+-----------+------+--------+------------------+\n",
      "|summary|campaign_cd|         campaign_nm|            name_cvm|    global_code|         global_name|             regid|           service|        cvm|contur| product|              dayy|\n",
      "+-------+-----------+--------------------+--------------------+---------------+--------------------+------------------+------------------+-----------+------+--------+------------------+\n",
      "|  count|       4276|                4276|                4306|           4306|                4306|              4306|              4306|       4306|  4306|    4306|              4306|\n",
      "|   mean|       null|                null|                null|           null|                null| 65.01416627960985|120860.38015582653|       null|  null|    null|29.777055271713888|\n",
      "| stddev|       null|                null|                null|           null|                null|10.199976496260218| 45939.84912316928|       null|  null|    null| 5.520229359911304|\n",
      "|    min|CAMP1004589| 0316_RTM_Back_Voice|50 SMS в поездках...|BILL_0000001180|10 минут для звон...|                 0|               0.0|CVMB2B-1005|   b2c|Mob_core|                 0|\n",
      "|    max|CAMP1031287|b2b_turbo_button_...|       Черный список|            нет|                 нет|                71|        502981.066|        нет|   b2c|     нет|                90|\n",
      "+-------+-----------+--------------------+--------------------+---------------+--------------------+------------------+------------------+-----------+------+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# получить статистику по таблице\n",
    "\n",
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10cc83-128a-49c9-bcd0-335b8bab8d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8-default",
   "language": "python",
   "name": "py38-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
